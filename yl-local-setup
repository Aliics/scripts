#! /usr/bin/env bash

function create_or_clear {
    if [ -d $1 ]; then
        rm -rf $1/*
    else
        mkdir $1
    fi
}

while getopts 'v:y:d:' OPT; do
    case $OPT in
        v) JAR_VERSION=$OPTARG;;
        y) YL_PARTITION=$OPTARG;;
        d) HBA_PARTITION=$OPTARG;;
        \?) exit 1;;
    esac
done

if [ -z "${JAR_VERSION}" ] || [ -z "${YL_PARTITION}" ] || [ -z "${HBA_PARTITION}" ]; then
    echo "Missing at least one of required options [v, y, d]"
    exit 1
fi

ENV_DIR="env-${JAR_VERSION}"

pushd "${HOME}/local-spark-testing" > /dev/null

create_or_clear ${ENV_DIR}
create_or_clear "${ENV_DIR}/yl-files"
create_or_clear "${ENV_DIR}/hba-files"
create_or_clear "${ENV_DIR}/output"
touch "${ENV_DIR}/yl_files.csv"
touch "${ENV_DIR}/hba_files.csv"

BUCKET="s3://ssp-datalake"
JAR_PATH="${ENV_DIR}/yl-win-job-${JAR_VERSION}.jar"
S3_JAR_PATH="${BUCKET}/emr-jars/yl-win-job/yl-win-job-${JAR_VERSION}.jar"
RAW_PATH="${BUCKET}/raw"
YL_PATH="${RAW_PATH}/yl-win/firehose/win/${YL_PARTITION}"
HBA_PATH="${RAW_PATH}/hba_raw_data/${HBA_PARTITION}"

echo "Copying jar from ${S3_JAR_PATH} -> ${JAR_PATH}"
aws s3 cp ${S3_JAR_PATH} ${JAR_PATH} || exit 2

echo "Populating yl-files with raw files in ${YL_PATH}"
aws s3 sync ${YL_PATH} "${ENV_DIR}/yl-files" || exit 2

echo "Populating hba-files with raw files in ${HBA_PATH}"
aws s3 sync ${HBA_PATH} "${ENV_DIR}/hba-files" || exit 2
mv "${ENV_DIR}/hba-files/kc_datacentre\=*/*" "${ENV_DIR}/hba-files/"
rm -rf "${ENV_DIR}/hba-files/kc_datacentre\=*/"

echo "Setting up csv files"
find . -wholename "./${ENV_DIR}/hba-files/*.json.gz" > "${ENV_DIR}/hba_files.csv"
find . -wholename "./${ENV_DIR}/yl-files/*.parquet" > "${ENV_DIR}/yl_files.csv"

SPARK_RUN="spark-submit --deploy-mode client 
    --conf spark.sql.files.ignoreCorruptFiles=true 
    --class org.adscale.YLWinJob 
    --executor-memory 1G 
    ${JAR_PATH} -y ${ENV_DIR}/yl_files.csv -d ${ENV_DIR}/hba_files.csv -o ${ENV_DIR}/output"

echo "Executing spark-submit: ${SPARK_RUN}"
eval $SPARK_RUN || exit 3

popd > /dev/null

